{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shakir-ahmed1/colab_notebooks/blob/main/tigrigna_stt/Tigrigna_STT_finetunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7seAfyBBSynq",
        "outputId": "71ff462e-4323-48a9-aefb-86652681a945"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Setup & Dependencies ====================\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install torchaudio librosa datasets jiwer evaluate\n",
        "!apt-get install -y ffmpeg\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7BJBpweN7ID",
        "outputId": "a2b27c57-80ba-470c-a748-5e522fb12bf3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-gswb0kzt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-gswb0kzt\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=cb0d34daadc04a94967a9d3a5d3396ac3c5494e63c0650dd5dbe93d7f039903e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oep8ue3e/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer, evaluate\n",
            "Successfully installed evaluate-0.4.4 jiwer-4.0.0 rapidfuzz-3.13.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EsgfaWbSgtH",
        "outputId": "86e5a8c0-b72b-4dcd-a57a-9859dff255a8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing 'unrar' utility...\n",
            "'unrar' installed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1XmUAExgUKk0P4Qxhbe0HRqiYRNQIErgX\n",
            "From (redirected): https://drive.google.com/uc?id=1XmUAExgUKk0P4Qxhbe0HRqiYRNQIErgX&confirm=t&uuid=a24ddda6-606a-4d5a-b944-1449730efebb\n",
            "To: /content/preprocessed_dataset.rar\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.1G/10.1G [01:58<00:00, 84.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded file to /content/preprocessed_dataset.rar\n",
            "Extracting /content/preprocessed_dataset.rar to /content/preprocessed_dataset\n",
            "Extracted /content/preprocessed_dataset.rar to /content/preprocessed_dataset\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import gdown\n",
        "\n",
        "def install_unrar():\n",
        "    print(\"Installing 'unrar' utility...\")\n",
        "    try:\n",
        "        !apt-get update > /dev/null 2>&1\n",
        "        !apt-get install unrar -y > /dev/null 2>&1\n",
        "        print(\"'unrar' installed successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error installing 'unrar': {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def download_from_drive(share_url, output_path):\n",
        "    try:\n",
        "        file_id = share_url.split('/d/')[1].split('/')[0]\n",
        "        gdown.download(f'https://drive.google.com/uc?id={file_id}', output_path, quiet=False)\n",
        "        print(f\"Downloaded file to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading from Google Drive: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def extract_rar_file(file_path, extract_to):\n",
        "    print(f\"Extracting {file_path} to {extract_to}\")\n",
        "    try:\n",
        "        if os.path.exists(extract_to):\n",
        "            shutil.rmtree(extract_to)\n",
        "            print(f\"Deleted existing directory at {extract_to}\")\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        result = subprocess.run(['unrar', 'x', '-y', file_path, extract_to + '/'],\n",
        "                                capture_output=True, text=True)\n",
        "        if result.returncode == 0:\n",
        "            print(f\"Extracted {file_path} to {extract_to}\")\n",
        "        else:\n",
        "            raise RuntimeError(result.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {file_path}: {e}\")\n",
        "\n",
        "install_unrar()\n",
        "\n",
        "# rar_url = 'https://drive.google.com/file/d/1JR3CBp_gWAv5FtgSbb9eM76k63ivxNJh/view?usp=sharing' # preprocessed tigrigna dataset\n",
        "# rar_url = 'https://drive.google.com/file/d/1225Iece2OEAGwYVLlbfJ1eWBhMDkbaq1/view?usp=sharing' # AmharicSpeechCorpus Magdeburg\n",
        "rar_url = 'https://drive.google.com/file/d/1XmUAExgUKk0P4Qxhbe0HRqiYRNQIErgX/view?usp=drive_link'\n",
        "\n",
        "local_rar_path = '/content/preprocessed_dataset.rar'\n",
        "extract_to_path = '/content/preprocessed_dataset'\n",
        "\n",
        "download_from_drive(rar_url, local_rar_path)\n",
        "extract_rar_file(local_rar_path, extract_to_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Dataset & Preprocessing ====================\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import WhisperTokenizer, WhisperFeatureExtractor, WhisperForConditionalGeneration\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "import evaluate\n",
        "import librosa\n",
        "from IPython.display import clear_output\n",
        "from torch.cuda.amp import autocast, GradScaler  # Added for mixed precision\n",
        "\n",
        "shared_drive_dir = \"/content/drive/MyDrive/shared_with_me/whisper_lora_tigrigna\"\n",
        "os.makedirs(shared_drive_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(shared_drive_dir, \"checkpoint.pth\")\n",
        "\n",
        "dataset_dir = '/content/preprocessed_dataset/joined_dataset/dataset'\n",
        "tsv_file_path = os.path.join(dataset_dir, 'data.tsv')\n",
        "audio_dir = os.path.join(dataset_dir, 'recordings')\n",
        "\n",
        "if not os.path.exists(tsv_file_path) or not os.path.exists(audio_dir):\n",
        "    print(\"Dataset missing. Exiting.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "df = pd.read_csv(tsv_file_path, sep='\\t', encoding='utf-8')\n",
        "df['path'] = df['path'].apply(lambda x: os.path.join(audio_dir, os.path.basename(x)))\n",
        "df.rename(columns={'sentence': 'text'}, inplace=True)\n",
        "\n",
        "def custom_split_data(df):\n",
        "  # 1) Shuffle the entire corpus\n",
        "  df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "  # 2) Pull out only the Tigrigna rows\n",
        "  tig_df = df_shuffled[df_shuffled['lang'] == 'tig']\n",
        "\n",
        "  # 3) Compute how many samples = 1% of the whole corpus\n",
        "  test_n = max(1, int(len(df_shuffled) * 0.01))\n",
        "  #    (ensure at least one sample)\n",
        "\n",
        "  # 4) Sample that many *Tigrigna* rows for test\n",
        "  #    If you happen to have fewer Tigrigna rows than test_n, this will just take them all.\n",
        "  test_df = tig_df.sample(n=min(test_n, len(tig_df)), random_state=42)\n",
        "\n",
        "  # 5) Everything else goes to train\n",
        "  train_df = df_shuffled.drop(test_df.index).reset_index(drop=True)\n",
        "\n",
        "  # 6) Convert to HuggingFace Dataset objects\n",
        "  train_data = Dataset.from_pandas(train_df)\n",
        "  test_data  = Dataset.from_pandas(test_df)\n",
        "  return train_data, test_data\n",
        "\n",
        "train_data, test_data = custom_split_data(df)\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language='amharic', task='transcribe')\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to('cuda')\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"out_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "class WhisperTrainingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, max_len=300):\n",
        "        self.dataset = dataset\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        try:\n",
        "            audio_data, _ = librosa.load(item['path'], sr=16000)\n",
        "        except:\n",
        "            audio_data = np.zeros(16000)\n",
        "        input_features = feature_extractor(audio_data, sampling_rate=16000, return_tensors='pt').input_features[0]\n",
        "        labels = tokenizer(item['text'], padding=\"max_length\", max_length=self.max_len,\n",
        "                           truncation=True, return_tensors=\"pt\")\n",
        "        labels = labels[\"input_ids\"].masked_fill(labels['attention_mask'].ne(1), -100)[0][1:]\n",
        "        return {\"input_features\": input_features, \"labels\": labels}\n",
        "\n",
        "train_dataset = WhisperTrainingDataset(train_data)\n",
        "test_dataset = WhisperTrainingDataset(test_data)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# ==================== Evaluation & Checkpoint Functions ====================\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        input_features = batch[\"input_features\"].to('cuda')\n",
        "        labels = batch[\"labels\"].to('cuda')\n",
        "        with torch.no_grad(), autocast():  # Use autocast for evaluation\n",
        "            generated_tokens = model.generate(input_features=input_features, language='amharic', task='transcribe')\n",
        "        predictions += tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        references += tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return wer_metric.compute(predictions=predictions, references=references) * 100\n",
        "\n",
        "def save_checkpoint(model, optimizer, scaler, epoch, global_step, loss):  # Modified to include scaler\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'global_step': global_step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict(),  # Save scaler state\n",
        "        'loss': loss\n",
        "    }, checkpoint_path)\n",
        "    print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "def load_checkpoint(model, optimizer, scaler):  # Modified to include scaler\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scaler.load_state_dict(checkpoint['scaler_state_dict'])  # Load scaler state\n",
        "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']+1}, step {checkpoint['global_step']}\")\n",
        "        return checkpoint['epoch'], checkpoint['global_step'], checkpoint['loss']\n",
        "    print(\"No checkpoint found, starting fresh.\")\n",
        "    return 0, 0, None\n",
        "\n",
        "# ==================== Training ====================\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "scaler = GradScaler()  # Initialize GradScaler for mixed precision\n",
        "max_epochs = 2\n",
        "running_loss, running_wer = [], []\n",
        "global_step = 1\n",
        "steps_per_epoch = len(train_dataloader)\n",
        "checkpoint_interval = 100\n",
        "\n",
        "start_epoch, global_step, last_loss = load_checkpoint(model, optimizer, scaler)  # Pass scaler\n",
        "if start_epoch > 0:\n",
        "  start_epoch += 1\n",
        "if last_loss: running_loss.append(last_loss)\n",
        "\n",
        "batches_to_skip = global_step % steps_per_epoch if global_step > 0 else 0\n",
        "\n",
        "for epoch in range(start_epoch, max_epochs):\n",
        "    batch_iterator = iter(train_dataloader)\n",
        "    if epoch == start_epoch:\n",
        "        for _ in range(batches_to_skip): next(batch_iterator, None)\n",
        "\n",
        "    for batch in tqdm(batch_iterator, desc=f\"Epoch {epoch+1}\"):\n",
        "        model.train()\n",
        "        input_features = batch[\"input_features\"].to('cuda')\n",
        "        labels = batch[\"labels\"].to('cuda')\n",
        "        with autocast():  # Enable mixed precision\n",
        "            loss = model(input_features, labels=labels).loss\n",
        "        scaler.scale(loss).backward()  # Scale loss before backward\n",
        "        scaler.step(optimizer)  # Update optimizer with scaled gradients\n",
        "        scaler.update()  # Update scaler for next iteration\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        running_loss.append(loss.item())\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            plt.plot(running_loss)\n",
        "            plt.xlabel(\"Steps\")\n",
        "            plt.ylabel(\"Loss\")\n",
        "            clear_output(wait=True)\n",
        "            plt.show()\n",
        "\n",
        "        # if global_step % 100 == 0:\n",
        "        #     model.save_pretrained(os.path.join(shared_drive_dir, \"lora_model_amharic\"))\n",
        "        #     save_checkpoint(model, optimizer, scaler, epoch, global_step, loss.item())  # Pass scaler\n",
        "\n",
        "    save_checkpoint(model, optimizer, scaler, epoch, global_step, loss.item())  # Pass scaler\n",
        "    wer = evaluate_model(model, test_dataloader)\n",
        "    running_wer.append(wer)\n",
        "    print(f\"Epoch {epoch+1} WER: {wer:.2f}%\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ==================== Save Final Model ====================\n",
        "model.save_pretrained(os.path.join(shared_drive_dir, \"lora_model_amharic_final\"))\n",
        "print(\"‚úÖ Final model saved.\")\n",
        "\n",
        "# ==================== Inference Examples ====================\n",
        "print(\"\\nüîç Running inference on test samples...\")\n",
        "model.eval()\n",
        "for idx in range(min(5, len(test_data))):\n",
        "    item = test_data[idx]\n",
        "    audio_data, _ = librosa.load(item['path'], sr=16000)\n",
        "    input_features = feature_extractor(audio_data, sampling_rate=16000, return_tensors='pt').input_features.to('cuda')\n",
        "    with torch.no_grad(), autocast():  # Use autocast for inference\n",
        "        generated_tokens = model.generate(input_features, language='amharic', task='transcribe')\n",
        "    text_pred = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "    print(f\"‚ñ∂ Sample {idx}:\\nTrue : {item['text']}\\nPred.: {text_pred}\\n\")\n",
        "\n",
        "# ==================== Visualize Token Lengths ====================\n",
        "token_lengths = [len(tokenizer(text).input_ids) for text in train_data['text']]\n",
        "plt.hist(token_lengths, bins=30)\n",
        "plt.xlabel(\"Sentence Length (Tokens)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Token Length Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "djdoo6PUUi-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "1dcbd996-ef58-4a0c-e2b5-7b1fcea94400"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-3288550101.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # Initialize GradScaler for mixed precision\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No checkpoint found, starting fresh.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/7261 [00:00<?, ?it/s]/tmp/ipython-input-5-3288550101.py:160: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # Enable mixed precision\n",
            "Epoch 1:   0%|          | 7/7261 [00:09<2:35:51,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3288550101.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Enable mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Scale loss before backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update optimizer with scaled gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1692\u001b[0m                 )\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1695\u001b[0m             \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 )\n\u001b[1;32m   1187\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1189\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/modeling_whisper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_MzLyxwfbId",
        "outputId": "fae6b7ec-0604-4f28-fdf7-1991bee8c7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Running inference on 5 random test samples...\n",
            "‚ñ∂ Sample 1:\n",
            "True : ·ã∞·àµ·â≥·ã¨·àù ·â†·àù·ãµ·à© ·â∞·ãµ·àã·ã¨·àù ·â†·à∞·ãç ·àç·åÜ·âΩ ·äê·â†·à® ·ä†·àÅ·äï·àù ·àç·åÜ·âº ·àÜ·ã≠ ·àµ·àô·äù ·àò·äï·åà·ã¥·äï·àù ·ã®·àö·å†·â•·âÅ ·àù·àµ·åâ·äñ·âΩ ·äì·â∏·ãç ·âµ·àù·àÖ·à≠·â¥·äï ·àµ·àô ·å†·â¢·â£·äï·àù ·àÅ·äë ·â∏·àç ·ä†·âµ·â†·àâ·âµ·àù \n",
            "Pred.: ·ã∞·àµ·â≥·ã¨·àù ·â†·àù·ãµ·à© ·â∞·ãµ·àã·ã¨·àù ·â†·à∞·ãç ·àç·åÜ·âΩ ·äê·â†·à® ·ä†·àÅ·äï·àù ·àç·åÜ·âΩ ·ã®·àÜ·ã≠ ·àµ·àô·äù ·àò·äï·åà·ã¥·äï·àù ·ã®·àö·å†·â•·âÅ ·àù·àµ·åç·äñ·âΩ ·äì·â∏·ãç ·â∞·àù·à≠·â¥·äï ·àµ·àô ·å†·â¢·â£·äï·àù ·àÅ·äë ·â∏·àã·âµ ·â†·àâ·âµ·àù \n",
            "\n",
            "‚ñ∂ Sample 2:\n",
            "True : ·äê·åà·à≠ ·åç·äï ·àï·ãù·â§ ·ä≠·â•·à©·äï ·àà·àõ·ã≠·à®·â£ ·äê·åà·à≠ ·àà·ãà·å† ·à∞·àõ·ã´·âµ ·àÜ·ã≠ ·â†·ãö·àÖ ·â∞·ã∞·äê·âÅ ·ä•·åÖ·åç·àù ·ã∞·äï·åç·å°·äì ·â∞·äï·å•·âÄ·å° ·ã≠·àã·àç ·ä•·åç·ãö·ä†·â•·àî·à≠ ·àï·ãù·â§ ·àÅ·àà·â±·äï ·ä≠·çâ ·äê·åà·àÆ·âΩ ·ä†·ãµ·à≠·åà·ãã·àç·äì \n",
            "Pred.: ·äê·åà·à≠ ·åç·äï ·àï·ãù·â§ ·ä≠·â•·à©·äï ·àà·àõ·ã≠·à®·â£ ·äê·åà·à≠ ·àà·ãà·å† ·à∞·àõ·ã´·âµ ·àÜ·ã≠ ·â†·ãö·àÖ ·â∞·ã∞·äê·âÅ ·ä•·åÖ·åç·àù ·ã∞·äï·åç·å°·äì ·â∞·äï·âÄ·å•·âÄ·å° ·ã≠·àã·àç ·ä•·åç·ãö·ä†·â•·àî·à≠ ·àï·ãù·â§ ·àÅ·àà·â±·äï ·ä≠·çâ ·äê·åà·àÆ·âΩ ·ä†·ãµ·à≠ ·åà·ãã·àç·äì \n",
            "\n",
            "‚ñ∂ Sample 3:\n",
            "True : ·ä•·äê·àÜ ·àå·ãã·ãç·ã´·äï·äï ·ãà·äï·ãµ·àû·âª·âΩ·àÅ·äï ·ä®·ä•·àµ·à´·ä§·àç ·àç·åÜ·âΩ ·àò·ä´·ä®·àç ·ãà·àµ·åÑ·ä†·àà·àÅ ·ã®·àò·åà·äì·äõ·ãç·äï ·ãµ·äï·ä≥·äï ·ä†·åà·àç·åç·àé·âµ ·ã´·ã∞·à≠·åâ ·ãò·äï·ãµ ·àà·ä•·åç·ãö·ä†·â•·àî·à≠ ·ã®·â∞·à∞·å° ·àà·ä•·äì·äï·â∞ ·àµ·å¶·â≥ ·äì·â∏·ãç\n",
            "Pred.: ·ä•·äï·àÜ ·àå·ãã·ãç·ã´·ãç·ã´·äï·äï ·ãà·äï·ãµ·àû ·âª·âΩ·àÅ·äï ·ä®·ä•·àµ·à´·ä§·àç ·àç·åÜ·âΩ ·àò·ä´·ä®·àç ·à¶·àµ·åÉ·àà·àÅ ·ã®·àò·åà·äì·äõ·ãç·äï ·ä•·äï·ä≥·äì·åà·àç ·åç·àé·âµ ·ã´·ã∞·à≠·åâ ·ãò·äï·ãµ ·àà·ä•·åç·ãö·ä†·â•·àî·à≠ ·ã®·â∞·à∞·å° ·àà·ä•·äì·äï ·â∞·àµ·å¶·â≥·äì·â∏·ãç \n",
            "\n",
            "‚ñ∂ Sample 4:\n",
            "True : ·ãã·â± ·ãã·â¥ ·â†·â∞·à∞·äò·ãâ ·çä·àç·àû ·ã®·â∞·àà·ã´·ã© ·ä•·àù·äê·âµ ·â∞·ä®·â≥·ãÆ·âΩ ·àà·àÖ·ã≠·ãà·â≥·â∏·ãâ ·àµ·åã·âµ ·àã·ã≠ ·ãà·ãµ·âÄ·ãâ ·ä†·äï·ã± ·àà·ä†·äï·ã± ·â∞·åà·äï ·àÜ·äñ ·à≤·çã·àà·àù ·ã´·à≥·ã´·àç\n",
            "Pred.: ·ãã·â± ·ãã·âµ ·â†·âµ·à∞·äõ·ãç ·çä·àç·àù ·ã®·âµ·àà·ã´·ã© ·ä•·àù·äê·âµ·äï ·ã®·àö·ä®·â∞·àâ ·à∞·ãé·âΩ ·àà·àÖ ·ãà·â≥·â∏·ãç ·àµ·åã·âµ ·àã·ã≠ ·ãà·ãµ·âÄ·ãç ·ä†·äï·ã± ·àã·äï·ã± ·â∞·åà·äï ·àÜ·äñ ·à≤·çã·àà·àù ·ã´·à≥·ã´·àç \n",
            "\n",
            "‚ñ∂ Sample 5:\n",
            "True : ·ä†·ãµ·àõ·åØ ·â†·å£·àù ·àµ·àã·ã∞ ·äê·âÉ·âµ ·â£·ã∞·à®·â£·â∏·ãç ·ã®·âÖ·äì·âµ ·àò·äï·çà·àµ ·â∞·åà·çã·çç·â∞·ãç ·â†·àò·ä™·äì ·åà·åØ·âµ \n",
            "Pred.: ·ä†·ãµ·àõ·åß·ãç ·â†·å£·àù ·àµ·àã·ã∞·äê·âÉ·âµ ·â£·ã∞·à≠·â£·â∏·ãç ·ã®·âÖ·äì·âµ ·àò·äï·çà·àµ ·â∞·åà·çã·çç·â∂ ·â†·àò·ä™·äì ·åà·åß·âµ \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "# # ==================== Inference Examples ====================\n",
        "# print(\"\\nüîç Running inference on test samples...\")\n",
        "# model.eval()\n",
        "# for idx in range(min(5, len(test_data))):\n",
        "#     item = test_data[idx]\n",
        "#     audio_data, _ = librosa.load(item['path'], sr=16000)\n",
        "#     input_features = feature_extractor(audio_data, sampling_rate=16000, return_tensors='pt').input_features.to('cuda')\n",
        "#     with torch.no_grad():\n",
        "#         generated_tokens = model.generate(input_features, language='amharic', task='transcribe')\n",
        "#     text_pred = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "#     print(f\"‚ñ∂ Sample {idx}:\\nTrue : {item['text']}\\nPred.: {text_pred}\\n\")\n",
        "\n",
        "# ==================== Inference Examples ====================\n",
        "import random\n",
        "\n",
        "print(\"\\nüîç Running inference on 5 random test samples...\")\n",
        "model.eval()\n",
        "\n",
        "random_samples = random.sample(list(test_data), k=5)\n",
        "\n",
        "for idx, item in enumerate(random_samples):\n",
        "    try:\n",
        "        audio_data, _ = librosa.load(item['path'], sr=16000)\n",
        "        input_features = feature_extractor(audio_data, sampling_rate=16000, return_tensors='pt').input_features.to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(input_features, language='amharic', task='transcribe')\n",
        "\n",
        "        text_pred = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "        print(f\"‚ñ∂ Sample {idx + 1}:\\nTrue : {item['text']}\\nPred.: {text_pred}\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error processing sample {idx + 1}: {e}\")\n",
        "\n",
        "\n",
        "# ==================== Visualize Token Lengths ====================\n",
        "token_lengths = [len(tokenizer(text).input_ids) for text in train_data['text']]\n",
        "plt.hist(token_lengths, bins=30)\n",
        "plt.xlabel(\"Sentence Length (Tokens)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Token Length Distribution\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}